# Story 2.6: Ollama SDK Integration Test for Streaming

## Status
Draft

## Story
**As an** Ollama SDK user,
**I want** to verify streaming generation works with the SDK,
**so that** I can build real-time applications.

## Acceptance Criteria
1. Integration test uses Ollama SDK with stream=True parameter
2. Test collects all chunks from the stream
3. Test verifies each chunk has correct format
4. Test confirms final chunk has done=true
5. Test reconstructs full response from chunks
6. Test verifies streaming provides same result as non-streaming

## Tasks / Subtasks
- [ ] Set up streaming test infrastructure (AC: 1)
  - [ ] Create test_ollama_sdk_streaming() in integration tests
  - [ ] Configure async test environment
  - [ ] Set up chunk collection mechanism
  - [ ] Create streaming response mocks
- [ ] Implement chunk validation (AC: 2, 3)
  - [ ] Collect all chunks from stream
  - [ ] Verify each chunk is valid JSON
  - [ ] Check required fields in each chunk
  - [ ] Validate chunk sequence (incremental text)
- [ ] Test streaming lifecycle (AC: 4)
  - [ ] Verify first chunk has done=false
  - [ ] Verify intermediate chunks have done=false
  - [ ] Confirm final chunk has done=true
  - [ ] Test empty response in final chunk
- [ ] Response reconstruction test (AC: 5)
  - [ ] Concatenate response text from all chunks
  - [ ] Verify complete response matches expected
  - [ ] Check timestamp consistency across chunks
  - [ ] Validate model name consistency
- [ ] Compare streaming vs non-streaming (AC: 6)
  - [ ] Run same prompt with stream=false
  - [ ] Run same prompt with stream=true
  - [ ] Compare final text results
  - [ ] Document any differences
- [ ] Test edge cases and errors (AC: 1-4)
  - [ ] Test very short responses (1-2 chunks)
  - [ ] Test very long responses (100+ chunks)
  - [ ] Test stream interruption handling
  - [ ] Test empty prompt streaming

## Dev Notes

### Previous Story Insights
- Story 2.4 implements the streaming infrastructure
- Story 2.5 tests non-streaming, this validates streaming mode

### Data Models
Expected Ollama SDK streaming usage:
```python
import ollama

client = ollama.Client(host='http://localhost:11434')
stream = client.generate(
    model='llama2',
    prompt='Tell me a story',
    stream=True
)

# Iterate over chunks
for chunk in stream:
    print(chunk['response'], end='', flush=True)
    if chunk['done']:
        break

# Chunk format
{
    'model': 'llama2',
    'created_at': '2023-08-04T19:56:02.647Z',
    'response': 'Once',  # Incremental text
    'done': False
}
```

### API Specifications
[Source: architecture/ollama-api-implementation.md#post-apigenerate]
Streaming requirements:
- Newline-delimited JSON format
- Each chunk must be complete JSON
- Incremental response text
- Final chunk with done=true and empty response

### Component Specifications
Streaming test considerations:
- Async iteration over chunks
- Handle network delays between chunks
- Proper stream cleanup on completion
- Memory-efficient chunk processing

### File Locations
[Source: architecture/source-tree.md]
- Add to: `tests/integration/test_ollama_sdk_generate.py`
- Or create: `tests/integration/test_ollama_sdk_streaming.py`
- Reuse fixtures from `tests/conftest.py`

### Testing Requirements
Streaming-specific test needs:
- Async test functions with pytest-asyncio
- Mock streaming OpenAI responses
- Test chunk arrival timing
- Validate memory usage during streaming
- Test connection stability

### Technical Constraints
[Source: architecture/tech-stack.md]
- Handle async iteration properly
- Test with realistic chunk delays
- Ensure proper resource cleanup
- Test timeout scenarios

### Testing
#### Test Standards
[Source: architecture/coding-standards.md]
- Use async def test functions
- Mock OpenAI streaming responses
- Test various chunk sizes and delays
- Validate complete stream lifecycle

#### Specific Testing Requirements
- Test rapid chunk delivery
- Test slow chunk delivery (network delays)
- Test very large responses (1000+ chunks)
- Test unicode/emoji in streamed text
- Test stream cancellation
- Measure streaming performance overhead
- Test memory usage during long streams

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-23 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*