# Story 2.3: Translation Engine for Generate Responses

## Status
Draft

## Story
**As a** developer,
**I want** to translate OpenAI completion responses back to Ollama format,
**so that** Ollama clients receive expected response structure.

## Acceptance Criteria
1. Translation converts OpenAI chat completion to Ollama generate response
2. Response includes model, created_at, response text, and done flag
3. Timestamps are converted to ISO format
4. Token usage information is included if available
5. Translation handles both complete and streamed responses
6. Error responses maintain Ollama's expected structure

## Tasks / Subtasks
- [ ] Review response format compliance requirements (AC: 1-6)
  - [ ] Check architecture/ollama-api-analysis.md for response quirks
  - [ ] Review exact Ollama response format from OpenAPI spec
  - [ ] Note any special fields or formatting requirements
- [ ] Create response format contract tests (AC: 1-6)
  - [ ] Test response matches Ollama OpenAPI specification
  - [ ] Verify all required fields are present
  - [ ] Test optional fields handling
  - [ ] Validate against tools/validate_against_openapi.py
- [ ] Create translation function for generate responses (AC: 1, 2)
  - [ ] Create translate_generate_response() in src/services/translator.py
  - [ ] Extract text from OpenAI message content
  - [ ] Map model name from response
  - [ ] Set done flag appropriately
  - [ ] Ensure exact field names match Ollama spec
- [ ] Handle timestamp formatting (AC: 3)
  - [ ] Convert OpenAI timestamp to RFC3339 format
  - [ ] Include timezone offset (not just 'Z')
  - [ ] Ensure consistent timestamp format across responses
  - [ ] Validate format matches Ollama expectations exactly
- [ ] Map token usage information (AC: 4)
  - [ ] Extract prompt_tokens, completion_tokens, total_tokens
  - [ ] Map to Ollama duration fields if applicable
  - [ ] Handle missing usage data gracefully
  - [ ] Check if Ollama expects specific field names
- [ ] Support streaming response translation (AC: 5)
  - [ ] Create translate_generate_stream_chunk() function
  - [ ] Handle partial response chunks
  - [ ] Set done=false for intermediate chunks
  - [ ] Set done=true for final chunk
  - [ ] Validate each chunk format against spec
- [ ] Error response translation (AC: 6)
  - [ ] Create translate_error_response() function
  - [ ] Maintain Ollama error format
  - [ ] Include error details and correlation IDs
- [ ] Unit tests for response translation (AC: 1-6)
  - [ ] Test complete response translation
  - [ ] Test streaming chunk translation
  - [ ] Test timestamp formatting
  - [ ] Test error response translation
  - [ ] Verify against OpenAPI specification

## Dev Notes

### Previous Story Insights
- Story 2.1 defines the OllamaGenerateResponse model structure
- Story 2.2 handles request translation, this handles the reverse

### Data Models
[Source: architecture/data-models.md#ollamagenerateresponse]
- **OllamaGenerateResponse**:
  - model: str - Model that generated response
  - created_at: str - ISO timestamp
  - response: str - Generated text
  - done: bool - Completion flag

[Source: architecture/ollama-api-implementation.md#post-apigenerate]
Expected Ollama response format (non-streaming):
```json
{
  "model": "llama2",
  "created_at": "2023-08-04T19:56:02.647Z",
  "response": "The sky is blue because...",
  "done": true,
  "total_duration": 5589157167,
  "load_duration": 3013701500
}
```

Streaming chunk format:
```json
{"model":"llama2","created_at":"2023-08-04T19:56:02.647Z","response":"The","done":false}
```

### API Specifications
[Source: architecture/ollama-api-implementation.md#common-pitfalls-to-avoid]
- Use RFC3339 with timezone offset, not just 'Z'
- Streaming format: Newline-delimited JSON, not arrays
- Include duration fields when available
- Preserve model name exactly as requested

### Component Specifications
OpenAI response structure to translate from:
```python
# Non-streaming
{
    "id": "chatcmpl-...",
    "model": "gpt-4",
    "created": 1234567890,
    "choices": [{
        "message": {"role": "assistant", "content": "text"},
        "finish_reason": "stop"
    }],
    "usage": {"prompt_tokens": 10, "completion_tokens": 20}
}

# Streaming chunk
{
    "choices": [{
        "delta": {"content": "text"},
        "finish_reason": null
    }]
}
```

### File Locations
[Source: architecture/source-tree.md]
- Translation service: `src/services/translator.py`
- Unit tests: `tests/unit/test_translator.py`
- Streaming helpers: `src/utils/streaming.py`

### Testing Requirements
Response translation test cases:
- Complete response with all fields
- Streaming chunks (first, middle, last)
- Responses without usage data
- Error responses (rate limit, auth, etc.)
- Timestamp format validation
- Very long response text

### Technical Constraints
[Source: architecture/ollama-api-implementation.md#critical-api-compliance-guidelines]
- Must maintain exact timestamp format
- Stream chunks must be valid JSON
- Error format must match Ollama expectations
- Preserve all duration/timing fields

### Testing
#### Test Standards
[Source: architecture/coding-standards.md]
- Test file: `tests/unit/test_translator.py`
- Test both OpenAI SDK response objects and raw dicts
- Use freezegun or similar for timestamp testing
- Mock various OpenAI response scenarios

#### Specific Testing Requirements
- Test RFC3339 timestamp formatting
- Verify streaming chunk sequence
- Test with/without token usage data
- Error response formatting
- Test response text extraction from choices
- Verify done flag logic

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-23 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*