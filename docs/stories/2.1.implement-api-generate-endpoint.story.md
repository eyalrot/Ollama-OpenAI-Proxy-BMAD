# Story 2.1: Implement /api/generate Endpoint

## Status
Draft

## Story
**As an** Ollama SDK user,
**I want** to generate text using client.generate() method,
**so that** I can create AI-generated content through the proxy.

## Acceptance Criteria
1. POST /api/generate endpoint is implemented in FastAPI
2. Endpoint accepts Ollama generate request format with model, prompt, and options
3. Request is validated using Pydantic models
4. Endpoint supports both streaming and non-streaming modes based on request
5. Non-streaming responses return complete JSON response
6. Streaming responses use Server-Sent Events format

## Tasks / Subtasks
- [ ] Create Ollama generate request/response models (AC: 2, 3)
  - [ ] Define OllamaGenerateRequest in src/models/ollama.py
  - [ ] Define OllamaGenerateResponse in src/models/ollama.py
  - [ ] Add validation for required fields (model, prompt)
  - [ ] Add optional parameters (stream, options)
- [ ] Implement /api/generate router (AC: 1, 4)
  - [ ] Create src/routers/generate.py
  - [ ] Add POST /api/generate endpoint
  - [ ] Add request validation using Pydantic models
  - [ ] Handle both streaming and non-streaming modes
- [ ] Integration with translation service (AC: 4, 5, 6)
  - [ ] Call translation service for request/response conversion
  - [ ] Handle non-streaming responses as complete JSON
  - [ ] Handle streaming responses as Server-Sent Events
- [ ] Unit tests for generate endpoint (AC: 1-6)
  - [ ] Test request validation
  - [ ] Test non-streaming response format
  - [ ] Test streaming response format
  - [ ] Test error handling for invalid requests

## Dev Notes

### Previous Story Insights
No previous stories in Epic 2. This is the foundational endpoint for text generation.

### Data Models
[Source: architecture/data-models.md#ollamageneraterequest]
- **OllamaGenerateRequest**: model (str), prompt (str), stream (bool), options (Dict[str, Any])
- **OllamaGenerateResponse**: model (str), created_at (str), response (str), done (bool)

[Source: architecture/ollama-api-implementation.md#post-apigenerate]
Required request format:
```json
{
  "model": "llama2",
  "prompt": "Why is the sky blue?",
  "stream": false,
  "options": {
    "temperature": 0.7
  }
}
```

Non-streaming response format:
```json
{
  "model": "llama2",
  "created_at": "2023-08-04T19:56:02.647Z",
  "response": "The sky is blue because...",
  "done": true,
  "total_duration": 5589157167,
  "load_duration": 3013701500
}
```

Streaming response format (newline-delimited JSON):
```
{"model":"llama2","created_at":"2023-08-04T19:56:02.647Z","response":"The","done":false}
{"model":"llama2","created_at":"2023-08-04T19:56:02.647Z","response":" sky","done":false}
{"model":"llama2","created_at":"2023-08-04T19:56:02.647Z","response":"","done":true}
```

### API Specifications
[Source: architecture/ollama-api-implementation.md#critical-api-compliance-guidelines]
- Must validate against official Ollama API specification
- Use RFC3339 timestamp format with timezone offset
- Streaming format must be newline-delimited JSON, not arrays
- Both streaming and non-streaming modes required

### Component Specifications
[Source: architecture/source-tree.md]
- Router location: `src/routers/generate.py`
- Model definitions: `src/models/ollama.py`
- Integration with: `src/services/translator.py` and `src/services/openai_client.py`

### File Locations
[Source: architecture/source-tree.md]
- Main router: `src/routers/generate.py`
- Models: `src/models/ollama.py` (add generate models)
- Unit tests: `tests/unit/test_routers.py` (add generate tests)
- Integration tests: `tests/integration/test_streaming.py`

### Testing Requirements
[Source: architecture/coding-standards.md#core-standards]
- Test organization: `tests/{unit,integration}/test_*.py`
- All route handlers must be async
- Type hints required for all functions
- Unit tests must cover request validation, response formats, error handling
- Integration tests for streaming functionality

### Technical Constraints
[Source: architecture/tech-stack.md]
- Python 3.12 with FastAPI 0.109.0
- Use Pydantic 2.5.3 for data validation
- All route handlers must be async
- Virtual environment required for local development
- Must use existing OpenAI SDK wrapper service

### Testing
#### Test Standards
[Source: architecture/coding-standards.md]
- Test files in `tests/unit/` and `tests/integration/`
- Use pytest framework with pytest-asyncio for async testing
- All endpoint tests must be async
- Test both success and error cases
- Mock external dependencies (OpenAI service)

#### Specific Testing Requirements
- Unit tests: Request validation, response formatting, error handling
- Integration tests: End-to-end streaming functionality
- Contract tests: Validate against Ollama API specification
- Test both streaming and non-streaming modes
- Verify timestamp formats and response structure

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-23 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*