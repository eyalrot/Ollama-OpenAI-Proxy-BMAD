# Story 2.7: Error Handling for Generation

## Status
Complete

## Story
**As a** developer,
**I want** comprehensive error handling for generation requests,
**so that** clients receive appropriate error messages.

## Acceptance Criteria
1. Rate limit errors (429) are properly translated to Ollama format
2. Authentication errors (401) return appropriate Ollama error structure
3. Model not found errors are handled with clear messages
4. Network timeouts are caught and reported
5. Invalid request parameters return 400 with details
6. All errors include correlation IDs for debugging

## Tasks / Subtasks
- [x] Review error format requirements (AC: 1-6)
  - [x] Check architecture/ollama-api-analysis.md for error format
  - [x] Review Ollama error structure from OpenAPI spec
  - [x] Note any special error fields or formatting
  - [x] Check how Ollama SDK expects errors
- [x] Create error format contract tests (AC: 1-6)
  - [x] Test error format matches Ollama spec
  - [x] Verify error structure for different status codes
  - [x] Test streaming error format
  - [x] Validate with tools/validate_against_openapi.py
- [x] Define Ollama error response format (AC: 1-5)
  - [x] Create OllamaErrorResponse model in src/models/ollama.py
  - [x] Include error message, type, and details fields
  - [x] Add correlation ID field for tracking
  - [x] Document error response structure
  - [x] Ensure format matches Ollama spec exactly
- [x] Implement error translation functions (AC: 1-3)
  - [x] Create translate_error_response() in translator
  - [x] Map OpenAI error codes to Ollama format
  - [x] Handle rate limit errors (429)
  - [x] Handle authentication errors (401)
  - [x] Handle model not found errors
  - [x] Validate translated errors against spec
- [x] Add error handling to generate endpoint (AC: 1-6)
  - [x] Wrap OpenAI calls in try/except blocks
  - [x] Catch specific OpenAI exceptions
  - [x] Return appropriate HTTP status codes
  - [x] Include correlation IDs in all errors
- [x] Handle network and timeout errors (AC: 4)
  - [x] Set request timeout configuration
  - [x] Catch timeout exceptions
  - [x] Handle connection errors
  - [x] Implement retry logic for transient failures
- [x] Validate request parameters (AC: 5)
  - [x] Check required fields (model, prompt)
  - [x] Validate parameter types and ranges
  - [x] Return detailed validation errors
  - [x] Test edge cases (empty strings, nulls)
- [x] Error handling for streaming (AC: 1-6)
  - [x] Handle errors during stream processing
  - [x] Send error chunk with done=true
  - [x] Clean up resources on stream errors
  - [x] Test stream interruption scenarios
  - [x] Ensure error chunks match Ollama format
- [x] Unit tests for error handling (AC: 1-6)
  - [x] Test each error type translation
  - [x] Test correlation ID generation
  - [x] Test streaming error handling
  - [x] Test validation error messages
  - [x] Test with Ollama CLI error scenarios

## Dev Notes

### Previous Story Insights
- Stories 2.1-2.6 implement the happy path for generation
- This story ensures robust error handling across all scenarios

### Data Models
Expected Ollama error format:
```json
{
  "error": {
    "message": "Rate limit exceeded",
    "type": "rate_limit_error",
    "code": 429,
    "details": {
      "retry_after": 60
    }
  },
  "correlation_id": "req_12345",
  "model": "llama2",
  "created_at": "2023-08-04T19:56:02.647Z"
}
```

Common OpenAI errors to handle:
- RateLimitError (429)
- AuthenticationError (401)
- InvalidRequestError (400)
- APIConnectionError (network)
- Timeout (408)

### API Specifications
[Source: architecture/error-handling-strategy.md#external-api-errors]
Error handling requirements:
- Preserve original error information
- Add context for debugging
- Use correlation IDs throughout
- Log errors with appropriate levels

[Source: architecture/ollama-api-implementation.md#common-pitfalls-to-avoid]
- Error responses must maintain Ollama format
- Include correlation IDs for debugging
- Stream errors as final chunk with error details

### Component Specifications
Error handling flow:
1. Catch OpenAI SDK exceptions
2. Translate to Ollama error format
3. Add correlation ID and context
4. Return appropriate HTTP status
5. Log error with details

### File Locations
[Source: architecture/source-tree.md]
- Error utilities: `src/utils/errors.py`
- Error models: `src/models/ollama.py`
- Integration: `src/routers/generate.py`
- Tests: `tests/integration/test_error_handling.py`

### Testing Requirements
Error scenarios to test:
- Rate limiting (429)
- Invalid API key (401)
- Model not found
- Network timeout
- Invalid parameters
- Server errors (500)
- Streaming errors

### Technical Constraints
[Source: architecture/error-handling-strategy.md#logging-standards]
- Use structured logging
- Include request context
- Log at appropriate levels
- Avoid logging sensitive data

### Testing
#### Test Standards
[Source: architecture/coding-standards.md]
- Mock various error conditions
- Test error message clarity
- Verify HTTP status codes
- Test correlation ID propagation

#### Specific Testing Requirements
- Test with mocked OpenAI errors
- Verify error response structure
- Test streaming error handling
- Validate correlation ID format
- Test error logging output
- Verify no sensitive data in errors
- Test retry logic for transient failures

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-23 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
claude-3-5-sonnet-20241022

### Debug Log References
- Implemented comprehensive error handling framework
- Created custom exceptions and error mappers
- Added error tests for all scenarios

### Completion Notes List
- Successfully implemented error handling for generation endpoints
- Proper error mapping between Ollama and OpenAI formats
- All error scenarios covered with tests

### File List
- Created: src/ollama_openai_proxy/utils/errors.py
- Created: tests/integration/test_error_handling.py
- Modified: src/ollama_openai_proxy/routes/generate.py

## QA Results
*Results from QA Agent review will be populated here*