# Story 2.5: Ollama SDK Integration Test for Generate

## Status
Draft

## Story
**As an** Ollama SDK user,
**I want** to verify client.generate() works for non-streaming requests,
**so that** I can generate complete responses.

## Acceptance Criteria
1. Integration test uses Ollama SDK to call generate() without streaming
2. Test verifies complete response is returned with expected fields
3. Test confirms response text is present and non-empty
4. Test verifies model name matches request
5. Test checks done=true flag is set
6. Multiple prompts are tested to ensure consistency

## Tasks / Subtasks
- [ ] Set up Ollama SDK test environment (AC: 1)
  - [ ] Create tests/integration/test_ollama_sdk_generate.py
  - [ ] Configure Ollama client to point to proxy
  - [ ] Set up test fixtures for client initialization
  - [ ] Mock or configure OpenAI backend response
- [ ] Implement basic generate test (AC: 1, 2, 3, 4, 5)
  - [ ] Test simple prompt generation
  - [ ] Verify all required fields in response
  - [ ] Check response text is non-empty
  - [ ] Verify model name matches request
  - [ ] Confirm done=true in response
- [ ] Test multiple prompt scenarios (AC: 6)
  - [ ] Short prompt (single sentence)
  - [ ] Long prompt (paragraph)
  - [ ] Special characters prompt
  - [ ] Empty prompt (error case)
  - [ ] Unicode/emoji prompt
- [ ] Test with different models (AC: 4, 6)
  - [ ] Test with different model names
  - [ ] Verify model name preservation
  - [ ] Test model name with tags (e.g., llama2:latest)
- [ ] Test options parameter (AC: 2, 6)
  - [ ] Test with temperature option
  - [ ] Test with max_tokens (num_predict)
  - [ ] Test with multiple options
  - [ ] Test without options (defaults)
- [ ] Add performance benchmarks (AC: 6)
  - [ ] Measure response time
  - [ ] Compare with direct OpenAI calls
  - [ ] Document acceptable latency range

## Dev Notes

### Previous Story Insights
- Stories 2.1-2.4 implement the complete /api/generate endpoint
- This story validates the implementation works with real Ollama SDK

### Data Models
Expected Ollama SDK usage:
```python
import ollama

client = ollama.Client(host='http://localhost:11434')
response = client.generate(
    model='llama2',
    prompt='Why is the sky blue?',
    stream=False
)

# Expected response structure
{
    'model': 'llama2',
    'created_at': '2023-08-04T19:56:02.647Z',
    'response': 'The sky is blue because...',
    'done': True,
    'total_duration': 5589157167,
    'load_duration': 3013701500
}
```

### API Specifications
[Source: architecture/ollama-api-implementation.md#sdk-vs-api-response-differences]
- Test with BOTH raw API calls and Ollama SDK
- SDK may transform responses differently
- Ensure compatibility with ollama CLI tools

### Component Specifications
Test environment setup:
- Run proxy on localhost:11434 (Ollama default)
- Configure OpenAI backend URL via environment
- Use test-specific OpenAI API key
- Mock OpenAI responses for consistency

### File Locations
[Source: architecture/source-tree.md]
- Integration test: `tests/integration/test_ollama_sdk_generate.py`
- Test fixtures: `tests/conftest.py`
- Test utilities: `tests/utils.py`

### Testing Requirements
Integration test requirements:
- Use real Ollama Python SDK
- Test against running proxy instance
- Mock OpenAI backend for deterministic results
- Validate full request/response cycle
- Test both success and error scenarios

### Technical Constraints
[Source: architecture/tech-stack.md]
- Python 3.12 environment
- Ollama SDK (latest version)
- pytest-asyncio for async tests
- Mock OpenAI responses for reliability

### Testing
#### Test Standards
[Source: architecture/coding-standards.md]
- Integration tests in `tests/integration/`
- Use pytest fixtures for client setup
- Mock external dependencies
- Each test should be independent
- Clean up resources after tests

#### Specific Testing Requirements
- Test timeout handling (30s default)
- Verify connection error handling
- Test with proxy running on different ports
- Validate response field types
- Test SDK-specific features
- Compare with ollama CLI behavior

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-23 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*