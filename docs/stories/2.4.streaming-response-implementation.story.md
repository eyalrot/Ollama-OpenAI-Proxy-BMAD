# Story 2.4: Streaming Response Implementation

## Status
Draft

## Story
**As an** Ollama SDK user,
**I want** to receive streaming responses for real-time text generation,
**so that** I can see output as it's being generated.

## Acceptance Criteria
1. Streaming endpoint returns Server-Sent Events (SSE) format
2. Each chunk is a complete JSON object with Ollama format
3. Stream includes incremental response text in each chunk
4. Final chunk has done=true flag
5. Stream handles client disconnections gracefully
6. Errors during streaming are properly formatted and sent

## Tasks / Subtasks
- [ ] Implement SSE streaming infrastructure (AC: 1)
  - [ ] Create streaming response helper in src/utils/streaming.py
  - [ ] Set up proper SSE headers (text/event-stream)
  - [ ] Handle connection keep-alive
  - [ ] Implement chunked transfer encoding
- [ ] Create streaming response generator (AC: 2, 3, 4)
  - [ ] Create async generator for OpenAI stream processing
  - [ ] Convert each OpenAI chunk to Ollama format
  - [ ] Ensure each chunk is valid JSON
  - [ ] Add newline delimiter between chunks
  - [ ] Set done=true on final chunk
- [ ] Integrate streaming with generate endpoint (AC: 1-4)
  - [ ] Detect stream=true in request
  - [ ] Return StreamingResponse with SSE content type
  - [ ] Use async generator for chunk processing
  - [ ] Maintain consistent timestamp across chunks
- [ ] Handle disconnections and errors (AC: 5, 6)
  - [ ] Catch client disconnect exceptions
  - [ ] Clean up OpenAI stream on disconnect
  - [ ] Format streaming errors as Ollama JSON chunks
  - [ ] Send error chunk with done=true
- [ ] Unit and integration tests (AC: 1-6)
  - [ ] Test SSE format compliance
  - [ ] Test chunk JSON validity
  - [ ] Test client disconnection handling
  - [ ] Test error during streaming
  - [ ] Test complete stream reconstruction

## Dev Notes

### Previous Story Insights
- Story 2.1 implements the /api/generate endpoint that will use this streaming
- Story 2.3 provides the chunk translation logic via translate_generate_stream_chunk()

### Data Models
[Source: architecture/ollama-api-implementation.md#post-apigenerate]
Streaming response format (newline-delimited JSON):
```
{"model":"llama2","created_at":"2023-08-04T19:56:02.647Z","response":"The","done":false}
{"model":"llama2","created_at":"2023-08-04T19:56:02.647Z","response":" sky","done":false}
{"model":"llama2","created_at":"2023-08-04T19:56:02.647Z","response":"","done":true}
```

### API Specifications
[Source: architecture/ollama-api-implementation.md#common-pitfalls-to-avoid]
- Streaming format: Newline-delimited JSON, not arrays
- Each line must be valid JSON
- No commas between chunks
- Empty response field in final chunk with done=true

SSE Requirements:
- Content-Type: text/event-stream
- Cache-Control: no-cache
- Connection: keep-alive
- Each chunk prefixed with "data: " (optional for Ollama)

### Component Specifications
[Source: architecture/tech-stack.md]
- FastAPI StreamingResponse for SSE
- OpenAI SDK streaming with async iteration
- Uvicorn for HTTP streaming support

Streaming flow:
1. OpenAI SDK returns async iterator
2. Process each chunk through translator
3. Yield formatted JSON + newline
4. FastAPI handles chunked encoding

### File Locations
[Source: architecture/source-tree.md]
- Streaming utilities: `src/utils/streaming.py`
- Integration point: `src/routers/generate.py`
- Integration tests: `tests/integration/test_streaming.py`

### Testing Requirements
Streaming test scenarios:
- Short response (few chunks)
- Long response (many chunks)
- Client disconnects mid-stream
- Server error during streaming
- Network timeout handling
- Verify chunk boundaries

### Technical Constraints
[Source: architecture/coding-standards.md#critical-rules]
- All streaming code must be async
- Handle asyncio.CancelledError for disconnects
- No blocking operations in stream loop
- Proper cleanup of OpenAI resources

### Testing
#### Test Standards
[Source: architecture/coding-standards.md]
- Integration tests: `tests/integration/test_streaming.py`
- Use httpx or aiohttp for SSE client testing
- Mock OpenAI streaming responses
- Test with pytest-asyncio

#### Specific Testing Requirements
- Verify SSE format (optional "data: " prefix)
- Test JSON validity of each chunk
- Confirm newline delimiters
- Test done flag progression
- Measure streaming latency
- Test memory usage for long streams
- Verify proper connection cleanup

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-23 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*